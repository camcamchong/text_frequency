How Kids Code and How We Know:
An Exploratory Study on the Scratch Repository
Efthimia Aivaloglou Felienne Hermans
e.aivaloglou@tudelft.nl
f.f.j.hermans@tudelft.nl
ABSTRACT
Block-based programming languages like Scratch, Alice and Blockly are becoming increasingly common as introductory languages in programming education. There is substantial research showing that these visual programming environ- ments are suitable for teaching programming concepts. But, what do people do when they use Scratch? In this paper we explore the characteristics of Scratch programs. To this end we have scraped the Scratch public repository and retrieved 250,000 projects. We present an analysis of these projects in three different dimensions. Initially, we look at the types of blocks used and the size of the projects. We then investigate complexity, used abstractions and programming concepts. Finally we detect code smells such as large scripts, dead code and duplicated code blocks. Our results show that 1) most Scratch programs are small, however Scratch programs consisting of over 100 sprites exist, 2) programming abstrac- tion concepts like procedures are not commonly used and 3) Scratch programs do suffer from code smells including large scripts and unmatched broadcast signals.
Keywords
Scratch; block-based languages; programming practices; code smells; static analysis
1. INTRODUCTION
Scratch [20] is a programming language developed to teach children programming by enabling them to create games and interactive animations. The public repository of Scratch programs contains over 14 million projects. Scratch is a block-based language: users manipulate blocks to program.
Block-based languages have existed since the eighties [9], but have recently found adoption as tools for programming education. In addition to Scratch, also Alice [4], Blockly1
1 https://developers.google.com/blockly/
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full cita- tion on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re- publish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
ICER ’16, September 08-12, 2016, Melbourne, VIC, Australia
⃝c 2016ACM.ISBN978-1-4503-4449-4/16/09...$15.00 DOI: http://dx.doi.org/10.1145/2960310.2960325
and App Inventor [23] are block-based and aimed at novice programmers.
Several studies have shown that block-based languages are powerful as tools for teaching programming [15, 18, 5, 19]. Previous works involving static analysis of Scratch programs have evaluated the application of various programming con- cepts in Scratch projects [12, 16]. Recent works have focused on bad programming practices within Scratch programs [14], and automated quality assessment tools have been proposed for identifying code smells [8] and bad programming prac- tices [2, 16]. In a recent controlled experiment we found that long scripts and code duplication decrease a novice program- mer’s ability to understand and modify Scratch programs [10].
The goal of this paper is to obtain an understanding of how people program in Scratch by analyzing their program- ming artifacts, i.e., the saved and shared Scratch projects. With this we aim to quantitatively evaluate the use of pro- gramming abstractions and concepts. Moreover, knowing that bad programming habits and code smells can be harm- ful [10], we also want to explore whether they are common. To address this goal, we answer the following research ques- tions:
RQ1 What are the size and complexity characteristics of Scratch programs?
RQ2 Which coding abstractions and programming concepts and features are commonly used when programming in the Scratch environment?
RQ3 How common are code smells in Scratch programs?
Our study is based on data from the Scratch project repos- itory. By scraping the list of recent projects2, we have ob- tained 250,166 public Scratch projects and performed source code analysis on them. To the best of our knowledge, this is the first large-scale exploratory study of Scratch programs.
The contributions of this paper are as follows:
• Apublicdatasetof233,491non-emptyScratchprojects (Section 3.1)
• An evaluation of the data set in terms of size, com- plexity, programming concepts and smells (Section 4)
• A discussion of the implications of our findings for ed- ucational programming language designers (Section 5)
2https://scratch.mit.edu/explore/projects/all/
Software Engineering Research Group Delft University of Technology Mekelweg 4, 2628 CD, Delft, the Netherlands
   rAloeCwtaMiontshaaecrknsnotonowedxloecdlsugose,isvfoet,rhraGotoytvahelitrsyn-cmforennetrtripibguhrtitpotnosepwsuaobsnlilasyuh.thoorredprorduccoe-atuhtihsoaretidclbey, oarntoemal- ployee, contractor or affiliate of a national government. As such, the Government
53
 
2. RELEVANT SCRATCH CONCEPTS
Scratch is a block-based programming language aimed at children, developed by MIT. Scratch can be used to create games and interactive animations, and is available both as a stand-alone and as a web application. The main concepts in the Scratch programming environment are (we refer the reader to [3] for an extensive overview):
Sprites Scratch code is organized by ‘sprites’: two dimen- sional pictures each having their own associated code. Scratch allows users to bring their sprites to life in var- ious ways, for example by moving them in the plane, having them say or think words or sentences via text balloons, but also by having them make sounds, grow, shrink and switch costumes.
Scripts Sprites can have multiple code blocks, called scripts. It is possible for a single sprite to have multiple scripts initiated by the same event. In that case, all scripts will be executed simultaneously.
Events Scratch is event-driven: all motions, sounds and changes in the looks of sprites are initiated by events called Hat blocks3). The canonical event is the when Green Flag clicked, activated by clicking the green flag at the top of the user interface. In addition to the green flag, there are a number of other events possible, including key presses, mouse clicks and input from a computer’s microphone or webcam.
Signals Events within Scratch can be user-generated too: users can broadcast a message, for example when two sprites touch each other. All other sprites can then react by using the when I receive Hat block.
Custom blocks Scratch users can define their own blocks, which users can name themselves, called custom blocks. The creation of custom blocks in Scratch is the equiv- alent of defining procedures in other languages [16]. Because the term ‘procedures’ is common in related work, we will refer to custom blocks as ‘procedures’ in the remainder of this paper. Procedure parameters can be textual, numeric or boolean. When a user defines a procedure, a new Hat block called define appears, which users can fill with the implementation of their procedure.
3. RESEARCH DESIGN AND DATASET
The main focus of this study is to understand how peo- ple program in Scratch, by analyzing the characteristics of Scratch projects. To answer our three research questions, we conducted an empirical quantitative evaluation of project data we collected from the Scratch project repository. In the following paragraphs we describe the dataset, the pro- cess and the tools we used for analyzing it, and the methods we followed for detecting code smells.
3.1 Dataset
In order to collect the set of Scratch projects in the dataset we built a web scraping program. The scraping program, called Kragle, starts by reading the Scratch projects page2 and thus obtains the project identifiers of projects that were
3http://wiki.scratch.mit.edu/wiki/Hat Block
most recently shared. Subsequently, Kragle retrieves the JSON code for each of the listed projects.
We ran Kragle on March 2nd 2016 for 24 hours and, during that time, it obtained a little over 250,000 projects. Out of the 250,166, we failed to parse and further analyze 2,367 projects due to technical difficulties with the JSON files. Kragle, as well as all scraped projects and our analysis files are available.4
Once we obtained the Scratch projects, we parsed the JSON files according to the specification of the format5. This resulted in a list of used blocks per project, with the sprites and the stage of the project. We also cross referenced all blocks with the Scratch wiki to determine the shapes and the category of all blocks. For example, When Green Flag Clicked is a Hat block from the Events category.
3.2 Data analysis
All scraped project data, including the list of used blocks and parameters, were imported in a relational database. We used SQL queries, which are also made available,4 for filter- ing, aggregating and extracting all statistical data required to address our three research questions. We also randomly sampled and manually inspected edge cases in the results, for example empty or overly complex projects. Data for these cases are provided as part of the dataset.4
To answer RQ1, we measured the size of projects based on the number of blocks in scripts and sprites and we cal- culated descriptive statistics, which are presented in Section 4.1. For measuring the complexity of the scripts we used the McCabe cyclomatic complexity metric [13], a quantita- tive measure of the number of independent paths through a program’s source code. This is commonly calculated per script by counting the number of decision points in the script plus one. The decision points that Scratch supports are the if and the if else blocks.
For RQ2, we used the data on the code blocks and their categories to perform statistical analysis of applied program- ming abstractions and concepts. Similarly to [6] and [12], we consider the use of certain blocks to indicate that a pro- gramming abstraction or concept is being used in a certain project. In Section 4.2 we present the results related to the utilization of procedures, variables, loops, conditional state- ments, user interactivity and synchronization.
For RQ3, we focused on four types of code smells: dupli- cated code, dead code, large script and large sprite. In the duplicated code smell analysis, our first step was to define what we consider a code clone in the context of Scratch pro- gramming: a script that is composed of a set of blocks of the same type connected in the same way and is repeated within or across sprites of the same project. For the identi- fication of clones we did not take into account the values of the parameters that may be used in the blocks, so that two blocks that only differ in the values of parameters are con- sidered to be equal. We also investigated the case of clones with the same parameter values, and we refer to them as ex- act clones. The next step in the analysis was to determine the minimum size of the scripts that are considered clones instead of incidentally similar. We examined the number of detected clones for different script sizes and present the results in Figure 1. Based on this distribution, we opted to
4 https://github.com/TUDelftScratchLab/ScratchDataset
5http://wiki.scratch.mit.edu/wiki/Scratch File Format (2. 0)
      54
 
 1,000,000 100,000
80,000
50,000
10,000,000 1,000,000 100,000 10,000 1,000 100 10
1 1
45,000 40,000 35,000 30,000 25,000 20,000 15,000 10,000
70,000 10,000 60,000
1,000
40,000 100 30,000
10
11101001000 0
Clones across sprites Clones within sprites
Figure 1: Number of cloned scripts of different block sizes across and within sprites
Number of cloned scripts
Number of sprites with code
Figure 3: Number of sprites in the analyzed projects
10
Number of sprites Number of scripts
10,000
5,000 0
Figure 2: Size of sprites and scripts in number of blocks
adopt the number also used by the authors in [16], which is a minimum size of five blocks per script.
In the context of Scratch, we consider the long method and the large class smells as large script and large sprite smells respectively. For these two smells we use the number of blocks as the size metric. Figure 2 presents the number of blocks in scripts and sprites of our dataset. We used these statistics to split the dataset and retrieve the top 10% largest scripts and sprites, as is commonly done in both source code analysis [1] and analysis of end-user programming artifacts like spreadsheets [11]. Using that strategy, we set the thresh- olds for the calculation of the large script and large sprite: 18 blocks and 59 blocks respectively. The results we obtained using these thresholds are presented in Section 4.3.
4. RESULTS
In the following sections, for each of the research ques- tions, we describe the results obtained through the analysis of the 247,798 Scratch projects in our dataset.
4.1 Program Size and Complexity
The dataset contains a relatively small number of projects without any code: 14,307 (5.77%). Through random man- ual sampling we found that in some cases these projects con- tains only sprites and costumes, but no code, while other projects were entirely empty apart from the Scratch cat added by default. Since these projects are empty in terms of code, we excluded them from further analysis, leaving the final number of analyzed non-empty projects to 233,491.
5,000 0
100
1,000
Number of blocks
Number of scripts (in all sprites)
Figure 4: Number of scripts in the analyzed projects
50,000 45,000 40,000 35,000 30,000 25,000 20,000 15,000 10,000
Number of blocks
Figure 5: Number of blocks in the analyzed projects
10,000,000 1,000,000 100,000 10,000 1,000 100 10 1
Figure 6: McCabe cyclomatic complexity of the 4,049,356 analyzed scripts
55
20,000 10,000
Scripts
Projects
Projects
Projects
1024 2048 4096 8192
16384 32768 More
16 32 64
128 256 512
1248
Number of blocks in script
1 2 4 8 16 32 64 128 256 More
Cyclomatic complexity

mean min Q1 median Q3 max
 Size
Complexity Procedures
Programming concepts
Sprites with code per project
Scripts per project
Number of blocks per project
Blocks in Stage per project
Blocks in Sprites per project
Blocks in Procedures per project
McCabe Cyclomatic Complexity (CC) per script McCabe CC per procedure script
Procedures per project with procedures
Arguments per Procedure
Numerical arguments per procedure with arguments Text arguments per procedure with arguments Boolean arguments per procedure with arguments Calls per procedure
Scripts with calls per procedure
Variables per project
Scripts utilizing variable
Lists per project
Conditional statements per project
Loop statements per project
User input blocks per project
Broadcast-receive statements per project
5.68 1 1 17.35 1 2 154.55 1 12 4.80 0 0 115.57 0 10
2 5 525
5 12 3,038 29 76 34,622 0 3 2,613 26 68 34,613
34.17 0 0 0 1.58 1 1 1 3.75 1 1 2
11.50 1 1 2 0.95 0 0 0 1.73 0 1 1 0.28 0 0 0 0.13 0 0 0 2.14 0 1 1 1.13 0 1 1 2.06 0 0 0 4.97 1 1 3 0.55 0 0 0
10.02 0 0 0 7.65 0 1 2 4.77 0 0 1 8.57 0 0 0
0 20,552 1 246 4 183 6 847 1 53 2 22 1 24 0 14 2 526 1 59 1 340 5 1,127 0 319 3 5,950 5 2,503 4 1,889 2 2,460
    Table 1: Summary statistics of the dataset of 233,491 non-empty Scratch projects
In Table 1 we summarize the statistics for the analyzed metrics. We use the mean value and the five-number sum- mary to describe the dataset in terms of the number of sprites with code per project (including the stage sprite) and the number of scripts and blocks per project. Figures 3, 4 and 5 plot the distribution of these size metrics.
We find that the majority of Scratch projects are small; 75% of the projects have up to 5 sprites, 12 scripts and 76 blocks, while one fourth of the projects have up to 12 blocks. On the other end, 5% of the projects (11,712) have more than 18 sprites and 4.8% (11,214) consist of more than 500 blocks. The analysis also highlighted some surprisingly large projects: 135 with more than 300 sprites and even 30 projects with more than 20,000 blocks, whose Scratch identifiers are made available for further inspection.4
The number of blocks metric was further analyzed to un- derstand code organization in more depth. The majority of Scratch code—74.78% out of 36,085,654 blocks—is written within sprites. An additional 3.1% of the total blocks are found in the stage class. More interestingly, the remain- ing 22.11% are blocks within defined procedures, which are found in only 7.7% (17,979) of the pro jects. The pro jects that contain procedures use them a lot; almost half of their total blocks (48.81%) are within procedures.
We further analyzed the utilization frequency of the dif- ferent block shapes and categories, as defined in the Scratch documentation. Figures 7 and 8 present the results in terms of number of blocks from the total 36,085,654 blocks in the dataset projects. The most commonly used blocks are from the Control and Data categories. The Others category in- cludes the blocks related to procedure calls and arguments.
To understand the complexity of the Scratch projects in our dataset, we use the McCabe cyclomatic complexity. The results of this metric per script are plotted in Figure 6. The majority (78.33%) of 4,049,356 scripts contain no decision points, while 13.08% have a cyclomatic complexity of 2, con-
Figure 7: Number of blocks from each category in the analyzed projects
Figure 8: Number of blocks of each shape in the analyzed projects
8,000,000 7,000,000 6,000,000 5,000,000 4,000,000 3,000,000 2,000,000 1,000,000
0
               Stack Reporter Hat Cap C Boolean
      0 5,000,000
10,000,000
15,000,000
20,000,000
56
 
7,000 6,000 5,000 4,000 3,000 2,000 1,000
0
1 2 4 8 16 32 64 128 256 512 More
       Number of procedures
 Retrieved
Analyzed
Non-empty (used for statistics)
Projects with:
Procedures 17,979 Recursive procedures 1,052 Variables 73,577 Lists 9,358
Number of pro jects 250,166 247,798 233,491
%
7.70%
0.45% 31.51% 4.01% 39.81% 56.24% 77.18% 13.59% 29.57% 25.54% 10.14% 2.12% 25.93% 11.81% 0.87% 28.16% 29.77% 13.68%
  Conditional statements
User input blocks
Loop statements
repeat until <condition> broadcast - receive
Cloned scripts across sprites Cloned scripts within sprites Cloned procedures
Cloned blocks across sprites Exact clones across sprites Exact clones within sprites Dead code
Large scripts Large sprites
92,959 131,314 180,210
31,739 69,039 59,634 23,671
4,945 60,554 27,574
2,043 65,760 69,521 31,954
Figure 9: Number of procedures for the 17,979 projects that include at least one procedure
1,000,000 100,000 10,000 1,000 100 10 1
0 1 2 4 8 16 32 64 More
       Number of arguments
 Table 2: Elements and characteristics of the projects in the dataset
taining exactly one decision point. The complexity is higher, over 4, for 3.67% of the scripts. The analysis also highlighted 209 scripts with a cyclomatic complexity over 100 and up to 246.4 Cyclomatic complexity was greater (mean value of 3.32) in defined procedures, with 56.46% of the procedures having at least one decision point.
4.2 Programming Abstractions and Concepts
The first method for abstraction that we investigate are procedures. In the dataset we found 206,799 procedures in 17,979 (7.7%) projects. As summarized in Table 1, the projects that contain procedures have an average of 11.5 procedures, with 53.59% of these projects having up to 2. Figure 9 shows the distribution of procedures in projects. Regarding procedure arguments, we found that 55.57% have no arguments and 19.48% have only one (shown in Figure 10). The majority of procedure arguments (80.59%) are nu- meric, and the least used argument type is the boolean one— 6.23% of the total procedure arguments, found in 5.32% of the procedures.
The use of procedures in projects was further investigated through the use of procedure calls, summarized in Figure 11. Most procedures are called exactly once (62.32% of them) or twice (14.30%) and from exactly one script (85.92% of them). Examining the origin of procedure calls, we ob- served that most of the calls (56.09%) originate from other procedures, and 1.06% even originate from the same proce- dure, making them recursive calls. These recursive proce-
Figure 10: Number of arguments of the procedures in the dataset
dures are found in 1,052 projects, whose identifiers are made available.4
As shown in Table 2, almost one-third of the projects use variables and a small number (4.01%) use lists. The num- ber of variables that is being used is also limited, with only 7.48% of the projects having five or more variables. The distribution of variable and list utilization is shown in Fig- ure 12. Exceptional cases exist: the analysis highlighted 842 projects with more than 100 variables and with a maximum of 340. Examining the initialization of variables through the set <variable> to <value> blocks, we found that for 4.83% of all variables this was missing. While failing to ini- tialize a variable in Scratch will not result in a runtime error as in some other programming languages, correctly setting the initial state of the program is important [2].
Regardingprogramcontrolfeatures,conditionals(if <con-
   RQ1: The majority of Scratch projects are small and sim- ple: 75% of the projects have up to 5 sprites, 12 scripts and 76 blocks. The majority (78%) of scripts contain no decision points. Most code is written in sprites. There exist surprisingly large and complex projects.
 1,000,000 100,000 10,000 1,000 100 10 1
                   Procedure calls
57
Figure 11: Number of calls of each procedure in the dataset
 Procedures (log scale) Procedures (logarithmic scale) Projects

1,000,000 100,000 10,000 1,000 100 10 1
                   Number of variables and lists
1,000,000 100,000 10,000 1,000 100 10 1
0 1 2 4 8 16 32 64 128 256 More
       Number of cloned scripts
across sprites exact clones across sprites within sprites
   Figure 12: Number of used variables and lists
Figure 13: Cloned scripts in the dataset projects
100,000 10,000 1,000 100 10 1
2 4 8 16 32 64 128 256 512 1024More
       Number of copies of cloned scripts
across sprites exact clones across sprites within sprites
    Block
when <> key pressed
when this sprite clicked (Sensing) key <> pressed? (Sensing) ask <> and wait (Sensing) mouse down? (Sensing) <attrib> of <> (Sensing) mouse X (Sensing) mouse Y
when <sensor> > <value> (Sensing) video <> on <>
Projects Occurrences
 71,096 39,179 37,919 19,039
9,115 9,068 5,977 3,940
705 434
294,771 198,342 291,657
66,850
54,079 155,468 27,321 22,035 1,570 1,397
Figure 14: Number of copies of the identified clones
    RQ2: A small number of projects (8%) use procedures, but they use them a lot and for more complex code. Most procedures are called once or twice, from a single script which, in more than half of the cases, is another proce- dure. Recursive procedure calls exist in 1,052 (0.5% of the total) pro jects. One third of the pro jects use vari- ables, sometimes without initializing them. 40% of the projects contain conditional statements and 77% contain loops, but conditional loops are rarely used. More than half of the pro jects are interactive. 30% of the pro jects use broadcast and receive blocks.
 Table 3: Frequency of use of user input blocks in the 233,491 projects of the dataset
dition> then and if <condition> then else blocks) are used by 39.81% of the projects. Loops (blocks repeat <tim- es>,foreverandrepeat until <condition>)aremorecom- mon, used by 77.18% of the projects. The most common of the three is the forever block, accounting for 51.86% of all loops and the least common one is the repeat until <con- dition> block, accounting for 11.57% of all loops and used in 13.59% of the total projects.
Investigating user interactivity functionality, we found that 56.24% of the projects in the dataset contain user input blocks—an average of 8.48 blocks per such project. Table 3 lists the frequency of use of user input controls. We do not include the when Green flag pressed block here, as this is just used to start a Scratch program and hence cannot be considered input into the program. The most commonly used user input block is the when key pressed, found in 71,096 (30.45% of the total) projects. The most frequently used parameter for the key attribute is the space key, fol- lowed by the arrows and then the letters and numbers.
As detailed in Section 2, users can define their own events, using the blocks broadcast, broadcast and wait and when I receive. These blocks are used in 29.57% of the projects. broadcast and wait is rarely used, in 3.87% of the projects.
4.3 Code smells
The duplicated code smell is the first smell that we exam- ine. As explained in Section 3.2, we use 5 as the minimum number of blocks for the identified clones. In total, in the dataset we found 170,532 scripts cloned across sprites in 59,634 (25.54% of the total non-empty) projects. 726,316 copies of these scripts were found, making each clone being copied an average of 4.26 times. Figure 13 plots the distri- bution of clones across pro jects. The ma jority of pro jects contain up to two cloned scripts. Figure 14 plots the num- ber of copies of the identified clones. It is of interest that 79,378 (46.55%) of the identified clones are copied three or more times, and even in 585 cases from 411 projects they are copied more than 50 times and up to 974.4
We further inspected which of the identified clones were duplicated within the same sprite: 63,682 (37.34% of the total) clones, in 10.14% of all pro jects. 2.12% of the pro jects contain cloned procedure definitions, which were measured to 12,878 (7.55% of the total) clones. Exact clones were found in 11.81% of the total projects. Their total number
58
 Clones (log scale) Projects
Projects (log scale)

was 66,750 (39.14% of the total) clones. Exact clones in the same sprite were rare, found in 0.87% of the projects.
Apart from whole scripts, we also examined cases where scripts differ only in the first (Hat) block. This way we exam- ine if Scratch programmers assign the same functionality to handle different types of events. Cloned functionality blocks are found to be rare: without considering the first block, only 2,243 additional clones were found in 920 projects.
The second smell that we examine is the dead code smell. We identify four types of dead code: (1) procedures that are not invoked, (2) unmatched broadcast-receive messages, (3) code that is not invoked and (4) empty event scripts, i.e., scripts that contain an event block alone. Investigat- ing the first type, we find that a significant number of the defined procedures (13,036 or 5.06%) are not called in the projects. This is also shown in Figure 11 and it occurs in 2,079 projects. For the second type, we examined the broadcast-receive messages and found that they are not al- ways synchronized: 3,33% of the when I receive blocks were found to wait for a message that is never being broad- casted, while 4,4% of the broadcast blocks broadcast a mes- sage that is not being received. This lack of syncronization occurs in 18,669 (7.99% of the total) projects.
The third and fourth cases are incomplete scripts. Those are either never invoked due to the lack of a starting when <trigger> block from the Scratch Events or Control cat- egory, or are comprised of only a when <trigger> block without any functionality. A total of 322,475 scripts like that were found in 56,890 (24.36% of the total) projects. The majority of these scripts (86.6%) are scripts missing a Hat block. Examining the size of these dead scripts, 72.34% are composed of a single block. However, some dead scripts are considerably large; 2,358 dead scripts in 1,553 different projects have more than 30 blocks and up to 2,610.4
The number of projects exhibiting the dead code smell, considering all four types of dead code combined, is 65,760 (28.16% of the total projects).
Finally, we examine the large script and the large sprite smells. The thresholds we use for the identification of large scripts and large sprites are 18 blocks and 59 blocks respec- tively, as explained in Section 3.2. The number of projects exhibiting the large script smell, i.e., containing at least one script with 18 or more blocks, is 69,521 (29.77% of the total projects) and the number of projects with the large sprite smell is 31,954 (13.68%).
5. DISCUSSION 5.1 Implications
We believe a large scale study of programs like the cur- rent can help language designers to tailor their language. In this section we highlight directions in which our study could support language design. There are many other implications to be considered, which is why we have made our dataset public.
5.1.1 Popularity of different block types
Our analysis shows that some categories of blocks are rarely used, like the ‘Pen’ blocks, of which only 194,885 oc- cur within 19,090 (8.17% of the total) programs. Hence, in future changes to the language, ‘Pen’ blocks might be less important to users to support or maintain.
5.1.2 Dead code
In our analysis, we find that more than one quarter of the Scratch projects contain dead scripts. In a sense, the dead scripts are harmless, as they are not executed. How- ever, they do cause ‘visual clutter’ and might be distracting to novice programmers, as it might be hard to see which scripts are dead. In contrast to other visual educational languages, Scratch does not indicate scripts that are dead. LEGO Mindstorms, for example, does give the user feedback on dead blocks by making them gray.
Looking at the number of unconnected blocks, we hypoth- esize that Scratch programmers have a need for a separate workspace to store unconnected blocks temporarily. We en- vision that would be like the ‘backpack’, a Scratch feature meant to move scripts across sprites. In order to help novice programmers keep their code clean, the programming inter- face could actively encourage users to move unconnected blocks to that workspace when they exit the environment.
5.1.3 Exact clones between sprites
With occurrences in 11% of the Scratch projects in our dataset, the use of exactly identical clones between sprites is relatively common. In a sense, the Scratch users are not to blame for that, as Scratch does not support procedure calls between sprites, only within them. So in many cases there is no way to share the functionality other than by making a copy. We are not aware of the underlying rationale of the Scratch team that lead to this decision, however it seems that a large part of the Scratch users would use the functionality to call procedures between sprites.
5.1.4 Sharing of scripts and procedures
Investigating the use of clones between projects, we ob- serve that there are 1,700 scripts that are used in multiple projects, sometimes as often as in 1,600 different projects. This seems to indicate that there are common patterns in Scratch projects, which means it might be very beneficial to Scratch programmers if they could not only share their projects, but also share some of their functionality, for others to use, like a library. An example of such a library could be: functions for platforming games, including the movement of sprites, collision detection and the implementation of ‘lives’. This might empower new Scratch users to get started faster.
5.2 Insights for computing education
Our findings confirm that Scratch is mainly used for its intended purpose as a first-exposure programming environ- ment for creating simple programs and interactive anima- tions: the majority of programs are small and more than half are interactive and contain no conditional statements. The analysis also indicates that computational concepts like conditionals and variables are being applied —conditional statements are found in 40% of the projects and variables in 32%. The same does not hold for loops: even though 77% of the projects contain loops, only 14% contains conditional loops. We attribute the increased use of the forever loop
   RQ3: Code clones are found in 26% of the projects, with almost half of the clones copied three or more times, in the same or across sprites. 28% of the projects contain code that is never invoked, and thus exhibit the dead code smell. In some cases these scripts are large. The large script smell is found in 30% of the projects and the large sprite one in 14%.
 59
 
to the Scratch language design and we are skeptical over whether it indicates an understanding of loop concepts.
Only 8% of the projects contain procedures, and those use them a lot and for more complex code, which is an indication of use by more experienced programmers. This very essential programming concept is therefore not suffi- ciently applied, which can be attributed both to limitations imposed by the Scratch environment, like the local scope of procedures, and to the difficulty for internalizing certain computational thinking concepts before a certain age [21].
The high occurrence of cloned scripts (in 26% of the proj- ects) could be the result of the limited use of procedures. Other code smells are also frequently found: 28% of the projects include dead code and 30% have large scripts. Know- ing from prior research that long scripts and code duplica- tion decreases ability to understand and modify Scratch pro- grams [10], and that Scratch programmers tend to exhibit certain bad programming habits [14], we believe educating novice programmers on code quality concepts is an issue that should be further researched.
It must be noted that the scope of this study includes the programming artifacts alone. Our findings are limited by the lack of (1) process data and (2) data on the age and other characteristics related to the programmers. However, the project data that is available in our dataset can facili- tate further studies on computing education, and this is the reason that we are publishing it.
5.3 Threats to validity
A threat to the validity of this study is the fact that we did not scrape a random sample, but the most recent 250,000 projects. It could be the case that the programming habits of Scratch users are changing over time. However, we coun- terbalanced that by using a large dataset which comprises around 2% of all 14 million shared Scratch projects6.
Furthermore we use the number of blocks in the Scratch projects as a measure for the length of a program, while this does not exactly correspond to the ‘length’ of a program in lines, and there can be multiple Scratch blocks on one line. We however believe that the number of blocks is a good proxy for size, and we plan a future experiment in which we will compare ‘lines of Scratch code’ to ‘number of blocks’.
6. RELATED WORK
The evaluation of block-based languages in general, and Scratch in particular, as tools for programming education has received significant research attention during the past years. A number of studies have been carried out on the understanding of programming concepts and the program- ming practices of novice programmers in block-based envi- ronments, on the programming skills they develop, and on the quality of Scratch programs.
For example, a study on the internalization of program- ming concepts with Scratch with 46 students was presented in [15]. Concepts like loops, conditional loops, message pass- ing, initialization, variables and concurency were examined, and it was found that students had problems with the last three. In a later study with an equal set of subjects [14] the same authors identified two bad programming habits in Scratch, namely bottom-up development and extremely fine-grained programming. They connected the later to the
6 https://scratch.mit.edu/statistics/
reduced use of if-blocks and finite loops and the increased use of infinite loops, a finding that is verified by our study. In [22], 29 projects created by 60 students working in groups were evaluated based on a list of criteria related to program- ming concepts, code organization and usability design.
Large-scale analyses of Scratch projects have been per- formed using the dataset made available by the Lifelong Kindergarten Group at the MIT Media Lab, which contains data for Scratch projects created until 2012, when the web- based programming environment was introduced. In [24], this dataset was used for exploring the learning patterns of programmers in terms of block use over their first 50 pro jects. Dasgupta et al. investigated how pro ject remixing relates to the adoption of computational thinking concepts [6]. In [7], the use of programming concepts was examined in relation to the level of participation (commenting, remix- ing, etc), the gender, and the account age of 5,000 Scratch programmers.
Most related to our study for the second research question of programming abstractions and concepts is the work by Maloney et al. [12], who analyzed 536 Scratch projects for blocks that relate to programming concepts including loops, conditional statements, variables, user interaction, synchro- nization, and random numbers. Compared to their findings, our investigation reveals increased use of the first three con- cepts, and especially variables.
The Scratch automated quality analysis tools Hairball [2] and Dr. Scratch [17] are also related to our work on smell detection. The Hairball Scratch extension is a lint-like static analysis tool for Scratch that can detect initialization prob- lems and unmatched broadcast and receive blocks. In their work [16], Moreno and Robles extended Hairball to detect two bad programming habits in Scratch: not changing the default object names and duplicating scripts, and apply them for evaluating 100 projects from the Scratch repository. The results on script duplication are substantially different from ours—we find projects with script clones to appear half as frequently. The Dr. Scratch tool [17] includes bad naming, code duplication and dead code identification functionality, and also evaluates Scratch projects in terms of abstraction, parallelism, logical thinking, synchronization, flow control, user interactivity and data representation.
7. CONCLUSIONS
In this paper we presented a large-scale study on 247,798 projects we scraped from the Scratch repository. We ana- lyze these projects in terms of size, complexity, application of programming abstractions and utilization of programming concepts including procedures, variables, conditional state- ments, loops, and broadcast-receive functionality. We find that procedures and conditional loops are not commonly used. We further investigate the presence of code smells, in- cluding code duplication, dead code, long method and large class smells. Our findings indicate that Scratch programs suffer from code smells and especially from dead code and code duplication.
In addition to the findings presented in this paper, we provide as contributions the dataset that we used for our study, as well as the project identifiers and information on the edge cases that we found in the dataset in terms of size and number of procedures, variables, cyclomatic complexity, clones and dead code.4
 60
 
8. REFERENCES
[1] T. L. Alves, C. Ypma, and J. Visser. Deriving metric thresholds from benchmark data. In 26th IEEE International Conference on Software Maintenance (ICSM 2010), pages 1–10. IEEE Computer Society, 2010.
[2] B. Boe, C. Hill, M. Len, G. Dreschler, P. Conrad, and D. Franklin. Hairball: Lint-inspired Static Analysis of Scratch Projects. In Proceeding of the 44th ACM Technical Symposium on Computer Science Education, SIGCSE ’13, pages 215–220, New York, NY, USA, 2013. ACM.
[3] K. Brennan, C. Balch, and M. Chung. Creative Computing. Harvard Graduate School of Education, 2014.
[4] M. Conway, R. Pausch, R. Gossweiler, and
T. Burnette. Alice: A Rapid Prototyping System for Building Virtual Environments. In Conference Companion on Human Factors in Computing Systems, CHI ’94, pages 295–296, New York, NY, USA, 1994. ACM.
[5] S. Cooper, W. Dann, and R. Pausch. Teaching Objects-first in Introductory Computer Science. In Proceedings of the 34th SIGCSE Technical Symposium on Computer Science Education, SIGCSE ’03, pages 191–195, New York, NY, USA, 2003. ACM.
[6] S. Dasgupta, W. Hale, A. Monroy-Hern ́andez, and
B. M. Hill. Remixing as a pathway to computational thinking. In Proceedings of the 19th ACM Conference on Computer-Supported Cooperative Work & Social Computing, CSCW ’16, pages 1438–1449, New York, NY, USA, 2016. ACM.
[7] D. A. Fields, M. Giang, and Y. Kafai. Programming in the wild: Trends in youth computational participation in the online scratch community. In Proceedings of the 9th Workshop in Primary and Secondary Computing Education, WiPSCE ’14, pages 2–11, New York, NY, USA, 2014. ACM.
[8] M. Fowler. Refactoring: improving the design of existing code. Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA, 1999.
[9] E. Glinert. Towards ”Second Generation” Interactive, Graphical Programming Environments. In Proceedings of the IEEE Workshop on Visual Languages, pages 61—70, 1986.
[10] F. Hermans and E. Aivaloglou. Do code smells hamper novice programming? In Proceedings of the International Conference on Program Comprehension, 2016. to appear.
[11] F. Hermans, M. Pinzger, and A. van Deursen. Detecting and refactoring code smells in spreadsheet formulas. Empirical Software Engineering, 20(2):549–575, 2015.
[12] J. H. Maloney, K. Peppler, Y. Kafai, M. Resnick, and N. Rusk. Programming by choice: Urban youth learning programming with scratch. In Proceedings of the 39th SIGCSE Technical Symposium on Computer Science Education, SIGCSE ’08, pages 367–371, New York, NY, USA, 2008. ACM.
[13] T. J. McCabe. A complexity measure. IEEE Trans. Software Eng., 2(4):308–320, 1976.
[14] O. Meerbaum-Salant, M. Armoni, and M. Ben-Ari.
[15]
[16]
[17]
[18]
[19]
[20]
[21]
[22]
[23]
[24]
Habits of programming in scratch. In Proceedings of the 16th Annual Joint Conference on Innovation and Technology in Computer Science Education, ITiCSE ’11, pages 168–172, New York, NY, USA, 2011. ACM. O. Meerbaum-Salant, M. Armoni, and M. M. Ben-Ari. Learning Computer Science Concepts with Scratch. In Proceedings of the Sixth International Workshop on Computing Education Research, ICER ’10, pages 69–76, New York, NY, USA, 2010. ACM.
J. Moreno and G. Robles. Automatic detection of bad programming habits in scratch: A preliminary study. In 2014 IEEE Frontiers in Education Conference (FIE), pages 1–4, Oct. 2014.
J. Moreno-Leo ́n, G. Robles, and M. Rom ́an-Gonza ́lez. Dr. Scratch: Automatic Analysis of Scratch Projects to Assess and Foster Computational Thinking. RED : Revista de Educacio ́n a Distancia, (46):1–23, Jan. 2015.
B. Moskal, S. Cooper, and D. Lurie. Evaluating the Effectiveness of a New Instructional Approach. In Proceedings of the SIGCSE technical symposium on Computer science education, 2005.
T. W. Price and T. Barnes. Comparing Textual and Block Interfaces in a Novice Programming Environment. In Proceedings of the Eleventh Annual International Conference on International Computing Education Research, ICER ’15, pages 91–99, New York, NY, USA, 2015. ACM.
M. Resnick, J. Maloney, A. Monroy-HernA ̃a􏰀ndez,
N. Rusk, E. Eastmond, K. Brennan, A. Millner,
E. Rosenbaum, J. Silver, B. Silverman, and Y. Kafai. Scratch: Programming for All. Commun. ACM, 52(11):60–67, Nov. 2009.
L. Seiter and B. Foreman. Modeling the learning progressions of computational thinking of primary grade students. In Proceedings of the Ninth Annual International ACM Conference on International Computing Education Research, ICER ’13, pages 59–66, New York, NY, USA, 2013. ACM.
A. Wilson, T. Hainey, and T. Connolly. Evaluation of computer games developed by primary school children to gauge understanding of programming concepts. In European Conference on Games Based Learning, page 549. Academic Conferences International Limited, 2012.
D. Wolber, H. Abelson, E. Spertus, and L. Looney. App Inventor: Create Your Own Android Apps. O’Reilly Media, Sebastopol, Calif, 1 edition edition, May 2011.
S. Yang, C. Domeniconi, M. Revelle, M. Sweeney,
B. U. Gelman, C. Beckley, and A. Johri. Uncovering trajectories of informal learning in large online communities of creators. In Proceedings of the Second (2015) ACM Conference on Learning @ Scale, L@S ’15, pages 131–140, New York, NY, USA, 2015. ACM.
61
 